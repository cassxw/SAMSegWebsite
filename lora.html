<!DOCTYPE HTML>
<!--
	Alpha by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>LoRA - SAMSeg</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/png" href="images/favicon/favicon-16x16.png" sizes="16x16">
		<link rel="icon" type="image/png" href="images/favicon/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="images/favicon/favicon.ico" sizes="any">
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
			<header id="header">
				<h1><a href="index.html">&#129504; SAMSeg</a> Honour's Research Project</h1>
				<nav id="nav">
					<ul>
						<li><a href="index.html" class="button icon solid fa-home">Home</a></li>
						<li>
							<a href="#" class="icon solid fa-angle-down button">Projects</a>
							<ul>
								<li><a href="u-sam.html">U-SAM</a></li>
								<li><a href="lora.html">LoRA</a></li>
							</ul>
						</li>
					</ul>
				</nav>
			</header>

			<!-- Main -->
			<section id="main" class="container">
				<header>
					<h2><strong>&#129504; SAMSeg-LoRA &#129504;</strong></h2>
					<h3>Fine Tuning SAM Through Low Rank Adaptation</h3>
					<p><i>By Tapera Chikumbu</i></p>
				</header>
				<div class="box">
					<h2><strong>&#10148; Introduction</strong></h2>
					<p>
						This research aimed to investigate how implementing Low-Rank Adaptation (LoRA) alters the performance of SAM on segmentation of intracranial meningiomas in brain MRIs, compared to the baseline SAM performance.
						LoRA adds trainable layers parallel to existing model parameters, potentially enhancing performance without increasing inference latency.
						Our modified SAM that implements LoRA was dubbed <i>SAMSEG-LoRA</i>. 
						Such an approach was particularly significant as it offered a way to adapt large, pre-trained models like SAM to specialized tasks without the computational cost of full fine-tuning.
					</p>
				</div>
				<div class="box">
					<h2><strong>&#10148; Design and Implementation</strong></h2>
					<p>
						LoRA layers were added to SAM's image encoder, keeping pre-trained parameters frozen while fine-tuning the new adapter layers.
						Various preprocessing techniques, including image slicing, centre cropping, and pixel value rescaling were implemented.
						The modified SAM was trained using the Adam optimizer, with performance evaluated using Dice scores and a combined Dice-Cross Entropy loss function.
					</p>
					<div class="row">
						<img src="images/LoRA_architecture.png" alt="Modified SAM Architecture with LoRA" style="width:100%; height:auto;">
						<p><strong>Figure 1:</strong> Modified SAM Architecture with LoRA Adapters</p>
					</div>
				</div>
				<div class="box">
					<h2><strong>&#10148; Results and Discussion</strong></h2>
					<p>
						The experiments compared the performance of SAM with different LoRA configurations (ranks 64, 128, and 256).
						LoRA_64 showed the lowest average losses, but all configurations displayed some unexpected patterns in training loss.
						A deeper analysis of LoRA_64 over 50 epochs showed initial improvements in both training and validation losses, followed by increasing losses after the 34th epoch.
						Dice score comparisons between the original SAM and LoRA-modified versions were inconclusive, with the LoRA models showing deteriorated performance.
					</p>
					<p>
						The following accuracy trends were observed during model training:
					</p>
					
					<span class="image fit"><img src="images/LoRA_results/64-valid_loss.png" alt="" /></span>
					<p><strong>Figure 2:</strong> Training and Validation Losses for LoRA-64</p>
					<p>
						Below are the predicted masks for LoRA-16, LoRA-32, and LoRA-64 models on sample 1100:
					</p>
					<div class="box alt">
						<div class="row gtr-50 gtr-uniform">
							<div class="col-4"><span class="image fit"><img src="images/LoRA_results/LoRA-16-mask-01100.png" alt="" /></span></div>
							<div class="col-4"><span class="image fit"><img src="images/LoRA_results/LoRA-32-mask-01100.png" alt="" /></span></div>
							<div class="col-4"><span class="image fit"><img src="images/LoRA_results/LoRA-64-mask-01100.png" alt="" /></span></div>
						</div>
					</div>

					<p>
						Reduction in training loss was not reflected in validation performance.
						Smaller ranks offered better performance but still fell below baselines.
						Visualisations of the predicted masks suggest the model is learning the entire image rather than tumour masks.
					</p>
					<table style="width:100%; border-collapse: collapse;">
						<tr>
							<th style="width:75%;">Model</th>
							<th style="width:25%;">Accuracy</th>
							<th style="width:25%;">PEFT Comparison</th>
							<th style="width:25%;">Vanilla Comparison</th>
						</tr>
						<tr>
							<td>LoRA-64</td>
							<td>2.1%</td>
							<td>-85.6%</td>
							<td>-82.0%</td>
						</tr>
						<tr>
							<td>LoRA-32</td>
							<td>3.9%</td>
							<td>-83.8%</td>
							<td>-80.1%</td>
						</tr>
						<tr>
							<td>LoRA-16</td>
							<td>4.9%</td>
							<td>-82.8%</td>
							<td>-79.1%</td>
						</tr>
					</table>
				</div>
				<div class="box">
					<h2><strong>&#10148; Conclusion</strong></h2>
					<p>
						The study demonstrates that SAM modified with LoRA adapters can be trained for improved performance on tumor segmentation.
						However, reliable performance comparisons between the original SAM and LoRA-modified versions could not be established.
						Further experimentation with different adapter ranks and finer hyper-parameter tuning is needed to shed more light on the effectiveness of this approach for medical image segmentation.
					</p>
				</div>

				<div class="box">
					<div class="documentation-box">
						<h2>&#8681; <strong>SAMSeg-LoRA Documentation</strong> &#8681;</h2>
						<ul>
							<li><a href="deliverables/SAMSeg - Literature Review - CHKTAP011.pdf" target="_blank" class="button icon solid fa-download">Literature Review</a></li>
							<li><a href="deliverables/SAMSeg - SAMSEG-LoRA Final Paper - CHKTAP011.pdf" target="_blank" class="button primary icon solid fa-download">Final Paper</a></li>
						</ul>
					</div>
				</div>

			</section>

			<!-- Footer -->
			<footer id="footer">
				<div class="footer-content">
					<h3>&#129504; <strong>SAMSeg:</strong> Improving SAM for Brain Tumour Segmentation</h3>
					
					<!-- Authors and Supervisors in Two Columns -->
					<div class="author-supervisor-container">
						<div class="authors">
							<p><strong>Authors:</strong></p>
							<ul>
								<li>Tapera Chikumbu - <a href="mailto:chktap011@myuct.ac.za">chktap011@myuct.ac.za</a></li>
								<li>Cassandra Wallace - <a href="mailto:wllcas004@myuct.ac.za">wllcas004@myuct.ac.za</a></li>
							</ul>
						</div>
						
						<div class="supervisors">
							<p><strong>Supervisors:</strong></p>
							<ul>
								<li>Patrick Marais - <a href="mailto:patrick@cs.uct.ac.za">patrick@cs.uct.ac.za</a></li>
								<li>Fred Nicolls - <a href="mailto:fred.nicolls@uct.ac.za">fred.nicolls@uct.ac.za</a></li>
							</ul>
						</div>
					</div>
					
					<!-- UCT Crest Section -->
					<div class="uct-crest">
						<img src="images/uct_logo.png" alt="UCT Crest" style="width: 100px; height: auto;">
					</div>
					
					<!-- University Info -->
					<ul class="university-info">
						<li>&copy; 2024, University of Cape Town, Computer Science Department</li>
						<li>Rondebosch, Cape Town, 7701, South Africa</li>
						<li>All Rights Reserved.</li>
					</ul>

					<!-- Copyright and Design Credits -->
					<ul class="copyright">
						<li>Design by <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>