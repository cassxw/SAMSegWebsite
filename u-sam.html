<!DOCTYPE HTML>
<!--
	Alpha by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>U-SAM - SAMSeg</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/png" href="images/favicon/favicon-16x16.png" sizes="16x16">
		<link rel="icon" type="image/png" href="images/favicon/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="images/favicon/favicon.ico" sizes="any">
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1><a href="index.html">&#129504; SAMSeg</a> Honour's Research Project</h1>
					<nav id="nav"> 
						<ul>
							<li><a href="index.html" class="button">Home</a></li>
							<li>
								<a href="#" class="icon solid fa-angle-down button">Projects</a>
								<ul>
									<li><a href="u-sam.html">U-SAM</a></li>
									<li><a href="lora.html">LoRA</a></li>
								</ul>
							</li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<section id="main" class="container">
					<header>
						<img style="display: block; margin: auto; width: 30%;" src="images/decoration/3d-brain.gif" alt="3D Brain Animation"/>

						<h2><strong>&#129504; U-SAM for Brain MRI &#129504;</strong></h2>
						<h3>A Hybrid Approach for Intracranial Meningioma Segmentation with SAM and U-Net</h3>
						
						<p><i>By Cassandra Wallace</i><br><a href="#introduction" class="button small" style="padding: 0 10px; margin-top: 10px">&#x27F1;</a></p>
					</header>

					<div id="introduction" class="box" style="scroll-margin-top: 40px">
						<h2><strong>&#10148; Introduction and Project Goals</strong></h2>
						<p>Welcome to U-SAM, a hybrid approach to medical image segmentation. Our innovative framework, U-SAM,
							synergises the powerful capabilities of the Segment Anything Model (SAM) with the precision of U-Net
							architectures. This unique combination is driven by our research question:</p>

						<blockquote>Can the integration of U-Net with SAM improve accuracy in intracranial meningioma segmentation
							for brain MRI, relative to Vanilla SAM on the same task?</blockquote>

						<p>At the heart of U-SAM lies the philosophy of "two brains working together". By leveraging SAM's general
							segmentation prowess alongside U-Net's specialisation in capturing intricate spatial details, we aim to
							create a more robust solution for medical image analysis. This collaboration not only seeks to harness the
							individual strengths of both models but also addresses the inherent limitations each faces when applied to
							intracranial meningioma segmentation.</p>
					</div>

					<div id="design" class="box">
						<h2><strong>&#10148; Design and Implementation</strong></h2>

						<h3>U-Net Architecture</h3>
						<p>Our 3D U-Net architecture is adapted by the Optimised U-Net proposed by Futrega et al. (NVIDIA), which introduces
							deep supervision through additional decoder levels closer to the output. This enhancement improves gradient flow,
							significantly boosting segmentation accuracy.</p>

						<p>While PEFT-SAM operates on a slice-by-slice basis, U-Net processes full 3D volumes, leveraging the spatial context
							inherent in entire datasets. Unlike SAM, which provides consistent outputs for identical inputs, U-Net's predictions
							can exhibit variability, showcasing its adaptability. We trained U-Net with the same hyperparameters and loss function
							strategy as PEFT-SAM, ensuring a fair comparison. Notably, during U-Net training, all parameters were updated, as opposed to
							the limited fine-tuning applied to PEFT-SAM's mask decoder.</p>

						<p>To ensure an equitable comparison between these models, we adjusted the U-Net segmentation process to account for the
							start and end points of the tumour volume within the axial plane. This emulation of SAM's bounding box prompt allows for
							a more balanced evaluation. Future research could explore the potential benefits of incorporating bounding boxes into U-Net's
							input channels, enhancing its performance further.</p>

						<h3>U-SAM-Combo</h3>
						<div class="content-container">
							<div class="text-content">
								<p>The U-SAM-Combo architecture harnesses the complementary strengths of PEFT-SAM and U-Net. In this framework, the input MRI is
									independently processed by both models, with their binary mask outputs merged to produce the final segmentation mask. We explored
									three combination strategies:</p>
								<ul>
									<li><strong>Union:</strong> This strategy aggregates predictions from both models, maximising coverage by simply merging the results.</li>
									<li><strong>Intersection:</strong> By focusing exclusively on regions where both models agree, this attempted to ensure precise segmentation outcomes.</li>
									<li><strong>Weighted:</strong> This method balances the predictions based on a preliminary accuracy evaluation of each model's performance against the ground truth segmentation mask. This research-focused approach emphasises the strengths of both frameworks, but currently lacks immediate clinical applicability due to the intermediate evaluation.</li>
								</ul>
							</div>
							<div class="image-content arch">
								<img src="images/U-SAM/u-sam-combo-arch.png" alt="U-SAM-Combo Architecture" />
							</div>
						</div>						
							
						<h3>U1-SAM2</h3>
						<p>U1-SAM2 presents an innovative integration of U-Net and SAM, where U-Net's output serves as a guidance mask for SAM's segmentation
							process. This sequential approach begins with processing the MRI through U-Net, generating an initial prediction that informs the
							subsequent segmentation decisions of PEFT-SAM.</p>

						<p>By incorporating U-Net's spatial understanding, U1-SAM2 refines SAM's segmentation capabilities, resulting in a more accurate final
							prediction. This is achieved through the utilisation of SAM's <code>mask_input</code> parameter in its prediction method. Despite being
							under-documented, this feature was pivotal for merging U-Net's output with SAM's processes.</p>

						<p>Importantly, SAM expects input masks containing unthresholded logits (floating-point confidence values) rather than binary outputs. To
							facilitate this seamless integration, we adapted tools from MicroSAM to convert U-Net's binary outputs into compatible logits, ensuring a
							smooth workflow between the two models.</p>
						<span class="image fit"><img src="images/U-SAM/u1-sam2-arch.png" alt="U1-SAM2 Architecture"/></span>

					</div>

					<div id="results" class="box">
						<h2><strong>&#10148; Results and Discussion</strong></h2>
						<h3>U-Net Training</h3>
						<p>The initial training of the U-Net model yielded a Dice Similarity Coefficient (DSC) of <strong>60%</strong>. Through a series of refinements
							involving adjustments in hyperparameters, loss functions, and data augmentations, we successfully improved the DSC to <strong>83.5%</strong>.
							These results demonstrate U-Net's ability to adapt to the complexities of medical imaging data, especially in handling 3D MRI volumes. The model
							was particularly effective in accurately delineating tumour boundaries in certain MRI modalities, but some difficulties arose in distinguishing
							the less visible tumour regions. As part of future work, incorporating more diverse and augmented datasets could further enhance U-Net's generalisation
							across various imaging conditions.</p>

						<img style="display: block; margin: auto; margin-bottom: 30px; width: 85%;" src="images/U-SAM/unet_training.png" alt="U-Net Training Progress"/>
						
						<h3>U-SAM-Combo Results</h3>
						<p>The U-SAM-Combo approach, which integrates U-Net and SAM predictions, was tested with three different combination strategies: union, intersection,
							and a weighted approach. The union method aggregates predictions from both models, aiming for a broader tumour coverage. It achieved an accuracy of 
							<strong>88.6%</strong>, improving upon both models but with some trade-offs in precision. The intersection method, which focused on the areas where both
							models agreed, performed slightly lower with an <strong>82.6%</strong> accuracy. The best results were seen with the weighted approach, where predictions
							were balanced based on each model's performance, achieving a remarkable <strong>91.8%</strong> accuracy. This demonstrates that leveraging the strengths
							of both U-Net and SAM in a hybrid manner can significantly boost overall accuracy, especially when individual predictions complement each other in terms
							of tumour delineation.</p>

						<h3>U1-SAM2 Results</h3>
						<p>Our experimentation with U1-SAM2, a novel iteration of U-SAM that incorporated different architectural tweaks, provided further insights. The model produced
							an overall DSC of <strong>85.1%</strong>, which represents a modest gain over the baseline models. However, the approach still fell short compared to the
							combined methods outlined earlier. We observed that while U1-SAM2 offered reliable tumour segmentation across multiple modalities, its performance varied
							depending on the complexity and visibility of the tumour region, especially in scans like T2w, where tumour contrast was not as pronounced. These results
							suggest that more nuanced strategies might be necessary to ensure consistently high performance across all MRI modalities.</p>

						<h3>Overall Results</h3>
						<p>Below is a comparative table of the DSC scores achieved by the different models and strategies employed during the project. The table also includes the
							percentage improvement or decline in accuracy compared to PEFT-SAM and Vanilla SAM, which serve as the primary baselines for this study.</p>

						<div class="table-wrapper">
							<table>
								<thead>
									<tr>
										<th><strong>Model</strong></th>
										<th><strong>Accuracy (DSC)</strong></th>
										<th><strong>Change from PEFT-SAM</strong></th>
										<th><strong>Change from Vanilla SAM</strong></th>
									</tr>
								</thead>

								<tbody>
									<tr>
										<td>U-SAM-Combo (Union)</td>
										<td>88.6%</td>
										<td>0.9%</td>
										<td>4.5%</td>
									</tr>
									<tr>
										<td>U-SAM-Combo (Intersection)</td>
										<td>82.6%</td>
										<td>-5.1%</td>
										<td>-1.5%</td>
									</tr>
									<tr>
										<td>U-SAM-Combo (Weighted)</td>
										<td><strong>91.8%</strong></td>
										<td><strong>4.1%</strong></td>
										<td><strong>7.7%</strong></td>
									</tr>
									<tr></tr>
										<td>U1-SAM2</td>
										<td>85.1%</td>
										<td>-2.6%</td>
										<td>1.0%</td>
									</tr>
								</tbody>
							</table>
						</div>

						<p>Below is a comparative table of the DSC scores achieved by the different models and strategies employed during the project. The table also includes the percentage improvement or decline in accuracy compared to PEFT-SAM and Vanilla SAM, which serve as the primary baselines for this study.</p>

						<img style="display: block; margin: auto; width: 85%; min-width: 500px" src="images/U-SAM/combos-visuals.png" alt="U-SAM-Combo and U1-SAM2 Visuals">

					</div>

					<div id="conclusion" class="box">
						<h2><strong>&#10148; Conclusion and Future Work</strong></h2>
						<p>U-SAM represents a key advancement in medical image segmentation, particularly for intracranial meningiomas. Our U-SAM-Combo approach, especially the Weighted strategy, demonstrated the most substantial improvement, achieving a <strong>7.7%</strong> increase in Dice Score over Vanilla SAM, while the Union method improved by <strong>4.5%</strong>. However, it's important to note that the Weighted method's reliance on intermediate evaluations with ground truth limits its practical application.</p>
					
						<p>In contrast, U1-SAM2 provided only a <strong>1%</strong> improvement over Vanilla SAM and underperformed compared to PEFT-SAM. Moreover, it introduced spurious pixels, highlighting the need for further refinement.</p>

						<p>Although Vanilla SAM already achieved good performance, even with our modifications, U-SAM still falls short of the near-perfect accuracy required for clinical applications. Nonetheless, our research goal was met: U-SAM successfully improved upon Vanilla SAM for intracranial meningioma segmentation. In conclusion, U-SAM shows potential for advancing medical imaging, but further refinement is needed to reach clinical-grade precision. Future work will focus on optimising these hybrid models for broader application and greater accuracy.</p>
						
						<p>Future research could focus on exploring new prompting methods, such as point-based or text-based inputs, to improve accuracy. Additionally, reversing the current approach in U2-SAM1, where SAM's predictions guide U-Net's segmentation, could lead to further enhancements. Broader applications of U-SAM in other medical imaging tasks, including organ segmentation and tumor detection, will also be investigated. Finally, with the release of SAM-2 in July 2024, which promises improved segmentation, its impact on U-SAM's performance will be assessed in future iterations.</p>
					</div>	
					
					<div class="documentation-box">
						<h2>&#8681; <strong>U-SAM Documentation</strong> &#8681;</h2>
						<ul>
							<li><a href="deliverables/SAMSeg - Literature Review - WLLCAS004.pdf" target="_blank" class="button">Literature Review</a></li>
							<li><a href="deliverables/SAMSeg - U-SAM Final Paper - WLLCAS004.pdf" target="_blank" class="button primary">Final Paper</a></li>
							<li><a href="deliverables/SAMSeg - U-SAM Code - WLLCAS004.zip" target="_blank" class="button">Code</a></li>
						</ul>
					</div>

				</section>

			<!-- Footer -->
				<footer id="footer">
					<div class="footer-content">
						<h3>&#129504; <strong>SAMSeg:</strong> Improving SAM for Brain Tumour Segmentation</h3>
						
						<!-- Authors and Supervisors in Two Columns -->
						<div class="author-supervisor-container">
							<div class="authors">
								<p><strong>Authors:</strong></p>
								<ul>
									<li>Tapera Chikumbu - <a href="mailto:chktap011@myuct.ac.za">chktap011@myuct.ac.za</a></li>
									<li>Cassandra Wallace - <a href="mailto:wllcas004@myuct.ac.za">wllcas004@myuct.ac.za</a></li>
								</ul>
							</div>
							
							<div class="supervisors">
								<p><strong>Supervisors:</strong></p>
								<ul>
									<li>Patrick Marais - <a href="mailto:patrick@cs.uct.ac.za">patrick@cs.uct.ac.za</a></li>
									<li>Fred Nicolls - <a href="mailto:fred.nicolls@uct.ac.za">fred.nicolls@uct.ac.za</a></li>
								</ul>
							</div>
						</div>
						
						<!-- UCT Crest Section -->
						<div class="uct-crest">
							<img src="images/uct_logo.png" alt="UCT Crest" style="width: 100px; height: auto;">
						</div>
						
						<!-- University Info -->
						<ul class="university-info">
							<li>&copy; 2024, University of Cape Town, Computer Science Department</li>
							<li>Rondebosch, Cape Town, 7701, South Africa</li>
							<li>All Rights Reserved.</li>
						</ul>

						<!-- Copyright and Design Credits -->
						<ul class="copyright">
							<li>Design by <a href="http://html5up.net">HTML5 UP</a></li>
							<li>Illustration by <a href="https://icons8.com/illustrations/author/zD2oqC8lLBBA">Icons 8</a> from <a href="https://icons8.com/illustrations">Ouch!</a></li>
						</ul>
					</div>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>